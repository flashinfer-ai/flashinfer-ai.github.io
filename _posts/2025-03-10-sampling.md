---
layout: post
title:  "Sorting-Free Rejection Sampling GPU-Kernels in FlashInfer for Faster Inference"
date:  2025-03-10
comments: true
author: Shanli Xing (UW), Zihao Ye (UW), Bohan Hou (CMU), Luis Ceze (UW), Tianqi Chen (CMU)
---

## Background

As vocabulary size grows in Large Language Models (LLMs), the sampling (token selection) process becomes a performance bottleneck. Sampling is key operator in LLM Inference Serving, the [sampling operators](https://docs.flashinfer.ai/api/sampling.html) in FlashInfer were first introduced in [v0.0.5](https://github.com/flashinfer-ai/flashinfer/releases/tag/v0.0.5)
and FlashInfer team has been improving the robustness and performance of the sampling operators since then. In this blog, we'll walk you through the algorithm and implementation details of sampling operators in FlashInfer.

## LLM Sampling

Sampling is the process that picks a specific next token from the vector of model logits (one per token). In practice, heuristics such as Top-P, Top-K, or Min-P thresholds are usually applied to pass  tokens with negligible probability, control generation behaviors, and enforce minimum probabilities.


<p align="center">
<img src="/assets/imgs/sampling_blog/Sampling_Portion.png" alt="The compute time break down highlighting the sampling process. In the vLLM 1xH100 configuration, our kernels reduce the overall sampling time by more than 50% across all three models." width="800"/>
<br>
<span style="color: gray; font-style: italic;">The compute time break down highlighting the sampling process. In the vLLM 1xH100 configuration, our kernels reduce the overall sampling time by more than 50% across all three models.</span>
</p>


1. **Top-K**
    
    Top-K sampling keeps only the $K$ tokens with the highest probabilities at each generation step. For example, if $K=50$, the model will ignore all tokens outside the top 50 likely candidates.
    
2. [**Top-P (Nucleus Sampling)**](https://arxiv.org/pdf/1904.09751)
    
    Top-P rather keeps the smallest set of tokens whose cumulative probability just exceeds a threshold $P$. For example, if $P=0.9$, you accumulate token probabilities in descending order until their sum is at least 0.9.
    
3. [**Min-P**](https://arxiv.org/pdf/2407.01082)
    
    Min-p filters out all tokens below a minimum threashold $p_\text{base} \times p_\text{max}$, where $p_\text{base}$
     is parameter and $p_\text{max}$ is the largest probability in the inputs. This helps eliminate extremely unlikely tokens while still respecting relative differences among the top candidates.
    

In practice, the combination of Top-K and Top-P is popular and used as the standard setting for LLM sampling. This allows for finer-grained control over the generation process. For example if we use the Top-K first filtering, we first limit the token set to the Top-K highest probabilities, and then apply a Top-P cutoff to filter the tail portion within those $K$ tokens.

> *FlashInfer provides both "Top-K First" and "Joint" filtering options, with the latter applying Top-K and Top-P simultaneously at each round. More on the* [doc](https://docs.flashinfer.ai/generated/flashinfer.sampling.top_k_top_p_sampling_from_probs.html)*.*
> 

A PyTorch implementation of these samplers might look like this:

```python
# vllm/vllm/model_executor/layers/sampler.py
def _apply_top_k_top_p(
    logits: torch.Tensor,
    p: torch.Tensor,
    k: torch.Tensor,
) -> torch.Tensor:
    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)

    # Apply top-k.
    top_k_mask = logits_sort.size(1) - k.to(torch.long)
    # Get all the top_k values.
    top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))
    top_k_mask = logits_sort < top_k_mask
    logits_sort.masked_fill_(top_k_mask, -float("inf"))

    # Apply top-p.
    probs_sort = logits_sort.softmax(dim=-1)
    probs_sum = probs_sort.cumsum(dim=-1)
    top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)
    # at least one
    top_p_mask[:, -1] = False
    logits_sort.masked_fill_(top_p_mask, -float("inf"))

    # Re-sort the probabilities.
    src = torch.arange(logits_idx.shape[-1],
                       device=logits_idx.device).expand_as(logits_idx)
    logits_idx_inv = torch.empty_like(logits_idx).scatter_(dim=-1,
                                                           index=logits_idx,
                                                           src=src)
    logits = torch.gather(logits_sort, dim=-1, index=logits_idx_inv)
    return logits
```

This code uses a combination of sorting, cumulative sums, and masking. While it is straightforward to follow,  it induces performance bottleneck especially for large vocab size.

In FlashInfer, we introduce the **Dual Pivot Rejection Sampling** algorithm and design multiple fused sampling kernels to fully leverage GPUs' parallel computing capabilities, ultimately achieving logarithmic time output. In this blog, we'll walk you through how we developed this algorithm integrating ideas from Inverse Sampling, Rejection Sampling, and binary search.

## Algorithm

### Inverse Transform Sampling

<p align="center">
<img src="/assets/imgs/sampling_blog/Inverse_Sampling.gif" alt="Inverse Transform Sampling. This animation illustrates the per-block process, and in practice the workload gets executed by blocks." width="800"/>
<br>
<span style="color: gray; font-style: italic;">Inverse Transform Sampling. This animation illustrates the per-block process, and in practice the workload gets executed by blocks.</span>
</p>

We begin with implementing a basic sampling kernel that selects tokens purely based on their probabilities, particularly in the GPU parallel computing context.

The method is **inverse transform sampling**, which draws samples from a probability distribution given its cumulative distribution function (CDF). As for the token samling process, the CDF would be the prefix sum of token probabilities. The algorithm proceeds like this:

1. **Draw a random** $u$ from $U\sim \text{Unif}(0,1)$.
2. **Compute the partial sums** (CDF) for each sampled token $j$ with probability $p_j$: $F_j=\sum^{j}_{i=1}p_i$.
3. **Identify the token** $k$ such that $F_{k-1} \leq u < F_k$ as the result.

Implementation side, the 2. and 3. parts are orchestrated for better parallelism: we scan the tokens in blocks, and within each block we:

1. Initialize a running total $\texttt{a}=0$. Compute the probability sum $\texttt{a\_local}$ . If $\texttt{a} + \texttt{a\_local}> u$, the sampled token lies in this block.
2. If not, we add $\texttt{a\_local}$ to $\texttt{a}$ and move on to the next block.
3. Once we know the correct block, we perform a prefix sum over its tokens to pinpoint the exact token index.

The per-block partial sum and prefix sums are computed leveraging [CUB collective primitives](https://docs.nvidia.com/cuda/cub/index.html) (now part of [CCCL](https://github.com/NVIDIA/cccl)) like [BlockReduce](https://nvidia.github.io/cccl/cub/api/classcub_1_1BlockReduce.html#_CPPv4I0_i_20BlockReduceAlgorithm_i_iEN3cub11BlockReduceE) and [BlockScan](https://nvidia.github.io/cccl/cub/api/classcub_1_1BlockScan.html#_CPPv4I0_i_18BlockScanAlgorithm_i_iEN3cub9BlockScanE) to maximize  efficiency.

### Rejection Sampling


<p align="center">
<img src="/assets/imgs/sampling_blog/Rejection_Sampling.gif" alt="Top-P Rejection Sampling. This animation illustrates the per-block process, and in practice the workload gets executed by blocks." width="800"/>
<br>
<span style="color: gray; font-style: italic;">Inverse Transform Sampling. This animation illustrates the per-block process, and in practice the workload gets executed by blocks. </span>
</p>


For more advanced strategies such as **Top-P sampling**, we use **rejection sampling** to restrict which tokens can be selected. Rejection sampling draws from a target distribution by comparing random samples against a threshold and discarding those that do not meet it.

Taking the Top-P sampling kernel as an example, here is a simplified look pf what happens:

1. **Initialize the pivot** to $0$, so initially all tokens are considered.
2. **Perform an inverse transform sampling pass** but ignoring tokens with probabilities below the current pivot. After sampling a token, **update the pivot** to that token’s probability.
3. **Compute the remaining probability** $\texttt{q}$ among tokens that still exceed this pivot:
    1. If $\texttt{q}$ remains greater than or equal to $\texttt{top\_p}$, another round is needed to raise the pivot further and reject more tokens.
    2. Otherwise, if it is below $\texttt{top\_p}$, we finalize the sampled token and mark success.
4. **Repeat** until successful or until a preset limit $\texttt{max\_top\_p\_rounds}$ is reached.

This algorithms works similar for Top-K, other than we’ll be checking the number of tokens exceeding the pivot against $\texttt{top\_k}$.

In practice, we find that this process rarely exceeds two rounds. It provides a substantial speedup by saving redundant CPU-GPU communications and on-CPU computations compared to native PyTorch implementations.

### Dual Pivot Rejection Sampling

## Evaluation

<p align="center">
<img src="/assets/imgs/sampling_blog/Throughput_Comparison_of_Different_Engine_Kernel.svg" alt="Throughput Comparison of Different Engine Kernel" width="800"/>
<br>
<span style="color: gray; font-style: italic;">Throughput Comparison of Different Engine Kernel.</span>
</p>

<p align="center">
<img src="/assets/imgs/sampling_blog/Sampling_Latency_Growth_with_Batch_Size.svg" alt="Sampling Latency Growth with Batch Size." width="800"/>
<br>
<span style="color: gray; font-style: italic;">Throughput Comparison of Different Engine Kernel.</span>
</p>