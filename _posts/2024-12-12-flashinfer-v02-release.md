---
layout: post
title:  "FlashInfer 0.2 - Efficient and Customizable Kernels for LLM Inference Serving"
date:  2024-12-16
comments: true
author: FlashInfer Community
---

<p align="center">
<img src="/assets/imgs/flashinfer-v02.jpg" alt="Generated by DALL-E" width="800"/>
<br>
Generated by DALL-E
</p>

After four months of development, we are thrilled to announce the release of **FlashInfer 0.2**. This major update introduces performance improvements, enhanced flexibility, and critical bug fixes. Key highlights of this release include:

- Faster sparse (page) attention with **FlashAttention-3 template**
- **JIT compilation** for attention variants
- Support for **Multi-head Latent Attention (MLA) decoding**

## FlashAttention-3 Template with Block/Vector-Sparsity

[FlashAttention-3](https://arxiv.org/pdf/2407.08608) brings a breakthrough optimization for Hopper GPUs by cleverly overlapping softmax and matrix multiplication. FlashInfer 0.2 integrates FA-3 templates, achieving significant improvements in prefill attention performance on Hopper architecture.
### Flexible Block-Sparsity and Vector-Sparsity

FlashInfer's standout feature is its highly flexible block-sparse FlashAttention implementation, supporting **any block size configuration**. Our PageAttention operators are implemented as **block-sparse attention kernels**, where `page_size` specifies the block's column count. At its finest granularity, FlashInfer supports **vector-sparsity**[^1] (`page_size=1`), allowing for precise memory management and efficient KV-Cache token pruning.

By leveraging [CuTE](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md)'s `CustomStride` and `ComposedLayout` abstractions, we have extended vector-sparsity to FlashAttention-3. Inspired by [Cutlass's gather/scatter convolution](https://github.com/NVIDIA/cutlass/tree/e1cd8c7866dd6de02b66a89879795e7d7301aacc/examples/59_ampere_gather_scatter_conv), this was achieved through an elegant modification to the producer's memory loading module.

### Performance Benchmark
We compared two attention implementations: PageAttention with `page_size=1` [^2] (use vector-sparse attention implementation) and variable-length dense attention [^3], benchmarking them under identical problem sizes across both FA-2 (v0.1.*) and FA-3 (v0.2) backends. Benchmarks used `head_dim=128`, `causal=True`, varying batch sizes `(B)` and sequence lengths `(L)` with Gaussian-initialized input Q/K/V tensors.

<p align="center">
<img src="/assets/imgs/fa3-template.png" alt="Performance comparison between dense/sparse attention on FA2&3 template" width="800"/>
<br>
Performance comparison between dense/vector-sparse attention on FA-2 and FA-3 templates on H100 SXM5, compiled with CUDA 12.4. y-axis: different settings, x-axis: achieved TFLOPs/s
</p>

**Results:** Vector-sparse attention achieves 90% of dense attention's throughput under identical conditions. The FA-3 backend consistently outperforms FA-2. Thanks to FlashInfer's stable API, upgrading from FA-2 to FA-3 requires no code changesâ€”just install FlashInfer 0.2. The reference benchmark script for reproducing these results is available [here](https://github.com/flashinfer-ai/flashinfer/blob/d7ac8e3ddc6623572c5c0e44af9e50a4c536a76c/benchmarks/bench_hopper_attention.py).

## JIT Compilation for Attention Customization

Inspired by [FlexAttention](https://pytorch.org/blog/flexattention/), FlashInfer 0.2 introduces customizable programming interface to compile different attention variants. We designed a modularized attention template in CUDA/Cutlass. Users can define custom attention variants by specifying functors such as `LogitsTransform`/`QueryTransform`/etc in an attention variant class. The class string will specialize our pre-defined Jinja templates and FlashInfer uses PyTorch's [JIT load](https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load) function to compile and cache these kernels. New variants like [FlashSigmoid](https://arxiv.org/pdf/2409.04431) can be implemented with minimal code. See our [JIT examples](https://github.com/flashinfer-ai/flashinfer/blob/124daea86fcdff4ba64e5b51337d81a46d6068cb/tests/test_jit_example.py) for more cases.

<p align="center">
<img src="/assets/imgs/jit.png" alt="JIT Compilation in FlashInfer 0.2" width="800"/>
<br>
Left: JIT workflow in FlashInfer. Right: Compile new attention variants.
</p>

In addition to supporting new attention variants, other benefits of supporting JIT in FlashInfer include:
- **Reduced wheel size:** The binary size of FlashInfer increases exponentially in recently releases because we pre-compile combination of all attention variants. We have to reduce specialization to make wheel size managable which harms kernel performance (as observed in [#602](https://github.com/flashinfer-ai/flashinfer/pull/602), FlashInfer v0.1.6's prefill performance is even worse than FlashInfer v0.1.1 because we move compile-time parameters to runtime which harms performance). FlashInfer v0.2 address the issue by just pre-compile a subset of **core** kernels ahead-of-time, while leaving most of the attention variants JIT compiled.
- **Light development:** No need to reinstall FlashInfer for minor CUDA changes, by installing FlashInfer in [JIT Mode](https://docs.flashinfer.ai/installation.html#install-from-source).

We have optimized the speed of JIT compilation by minimizing header dependencies and utilizing split compilation. As a result, all kernels for Llama models can be JIT-compiled within **15 seconds** on server-grade CPUs. For more details, check out our [JIT warmup scripts](https://github.com/flashinfer-ai/flashinfer/blob/main/tests/test_jit_warmup.py).

## Fused Multi-head Latent Attention (MLA) Decoding Kernel

**Multi-head Latent Attention (MLA)**, introduced in [Deepseek v2](https://arxiv.org/pdf/2405.04434), compresses the KV-Cache by projecting it into low-rank matrices. Achieving high throughput for MLA is challenging due to a lack of optimized kernels. FlashInfer community recently implemented an fused kernel with the **Matrix Absorption trick**, improving memory efficiency. See [#551](https://github.com/flashinfer-ai/flashinfer/pull/551) for detailed explanation.

<p align="center">
<img src="/assets/imgs/mla.png" alt="MLA" width="800"/>
<br>
MLA decode kernel workflow in FlashInfer
</p>

Future plans include accelerating MLA decoding with Tensor Cores, benefiting speculative decoding.

## CUDAGraph Compatibility for Variable-Length Inputs

FlashInfer 0.2 fixes prefill attention's incompatibility with CUDAGraph when query lengths vary during capture and replay stages, by accurately estimating upper resource bounds. CUDAGraphs now can be used to accelerate speculative decoding and chunked-prefil workloads with FlashInfer kernels.

## torch.compile Compatibility

FlashInfer 0.2 adheres to the [PyTorch Custom Operators Standard](https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/), ensuring compatibility with **torch.compile**.

## Packaging and CI/CD

We now provide [nightly builds](https://github.com/flashinfer-ai/flashinfer-nightly/releases) so users can test the latest features without waiting for stable releases.

## Other Notable Improvements

#### FusedAddRMSNorm Fix
Fixed numerical issues in `FusedAddRMSNorm`, which may cause bad outputs for some models.

#### Cutlass's SM90 Grouped-GEMM Integration
We integrated **Cutlass 3.5 SM90 Grouped-GEMM** into our [SegmentGEMM](https://docs.flashinfer.ai/api/gemm.html#flashinfer.gemm.SegmentGEMMWrapper) API, accelerating LoRA and MoE serving.

#### Non-Contiguous KV-Cache Support
KV-Cache can now utilize non-contiguous storage layouts, improving support for [offloading](https://github.com/flashinfer-ai/flashinfer/issues/506).

#### Faster `plan` Functions
`plan` functions now use non-blocking host-to-device memory transfers, improving performance. After FlashInfer v0.2, it's encouraged to pass **host tensors** instead of device tensors to reduce synchronization in the `plan` function.

#### KV-Cache Append Optimization
KV-Cache append throughput for small batch sizes was improved by parallelizing per element instead of per request. A new API, [get_batch_indices_positions](https://docs.flashinfer.ai/generated/flashinfer.page.get_batch_indices_positions.html), supports this. Note that we made some breaking changes to this API to accomodate different parallelization mode. See [our benchmark](https://github.com/flashinfer-ai/flashinfer/blob/124daea86fcdff4ba64e5b51337d81a46d6068cb/benchmarks/bench_append_paged_kv_cache.py) for the new API usage.

#### Standardized RoPE Interface
We standardized the RoPE interface to align with other frameworks. FlashInfer adopted **fp32 sin/cos** computations to avoid [numerical issues](https://arxiv.org/pdf/2411.13476).

## Roadmap
We appreciate the community's love and support. To enhance transparency, we've published our [development roadmap](https://github.com/flashinfer-ai/flashinfer/issues/675), where you can provide feedback and influence FlashInfer's future.

## Community Contributions
The number of contributors grew from 41 to 52 since v0.1.6. We thank the following developers for their contributions:

- [@yzh119](https://github.com/yzh119): JIT, FA-3 Template, and others
- [@abcdabcd987](https://github.com/abcdabcd987): torch.compile support, packaging
- [@nandor](https://github.com/nandor): Variable-length CUDAGraph support
- [@ur4t](https://github.com/ur4t): Packaging, CI/CD
- [@zhyncs](https://github.com/zhyncs): Nightly builds, CI/CD
- [@tsu-bin](https://github.com/tsu-bin): MLA decoding
- [@xslingcn](https://github.com/xslingcn): Cutlass Grouped-GEMM
- [@yuxianq](https://github.com/yuxianq): JIT, bug fixes
- [@LinHeLurking](https://github.com/LinHeLurking): Non-contiguous KV-Cache
- [@Abatom](https://github.com/Abatom): FusedAddRMSNorm fix
- [@jeejeelee](https://github.com/jeejeelee): Grouped-GEMM bug fixes
- [@mvpatel](https://github.com/mvpatel2000): Faster `plan` functions
- [@Ubospica](https://github.com/Ubospica): Pre-commit setup
- [@dc3671](https://github.com/dc3671): Improved unit tests
- [@Pzzzzz5142](https://github.com/Pzzzzz5142): JIT compilation fix
- [@reyoung](https://github.com/reyoung): Bug fixes
- [@xiezhq-hermann](https://github.com/xiezhq-hermann): ARM compilation fixes
- [@Bruce-Lee-LY](https://github.com/Bruce-Lee-LY): Performance optimizations
- [@francheez](https://github.com/francheez): Typo fixes

[^1]: [Efficient Tensor Core-Based GPU Kernels for Structured Sparsity under Reduced Precision](https://dl.acm.org/doi/pdf/10.1145/3458817.3476182)
[^2]: Use [BatchPrefillWithPagedKVCacheWrapper](https://docs.flashinfer.ai/api/prefill.html#flashinfer.prefill.BatchPrefillWithPagedKVCacheWrapper) API
[^3]: Use [BatchPrefillWithRaggedKVCacheWrapper](https://docs.flashinfer.ai/api/prefill.html#flashinfer.prefill.BatchPrefillWithRaggedKVCacheWrapper)

